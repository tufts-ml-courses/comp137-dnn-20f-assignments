{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network for Modeling Sentences\n",
    "\n",
    "In this task, we will use RNNs to model sentences. The task is to predict the next character in a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total of  4328 non-ascii chars are removed \n",
      "\n",
      "Data statistics:\n",
      "Number of sentences:  160000\n",
      "Maximum and minimum sentence lengths: 100 32\n",
      "Total number of characters: 10954565\n",
      "Vocabulary size:  95\n",
      "Chars in vocabulary and their frequencies:\n",
      "[('\\n', 160000), (' ', 1762678), ('!', 12100), ('#', 496), ('$', 1212), ('%', 450), ('&', 1366), (\"'\", 88729), ('(', 8734), (')', 8890), ('*', 4310), ('+', 123), (',', 33680), ('-', 20064), ('.', 108694), ('/', 1586), ('0', 11139), ('1', 10960), ('2', 7690), ('3', 3517), ('4', 2882), ('5', 4272), ('6', 2673), ('7', 2496), ('8', 2071), ('9', 2801), (':', 22223), (';', 607), ('<', 12), ('=', 103), ('>', 9), ('?', 48816), ('@', 34), ('A', 8259), ('B', 4063), ('C', 5317), ('D', 6787), ('E', 2239), ('F', 3232), ('G', 2668), ('H', 11482), ('I', 15839), ('J', 2999), ('K', 2315), ('L', 2612), ('M', 7724), ('N', 3017), ('O', 2211), ('P', 3722), ('Q', 1036), ('R', 2942), ('S', 7281), ('T', 15062), ('U', 1014), ('V', 720), ('W', 37161), ('X', 17), ('Y', 2381), ('Z', 149), ('[', 1), ('\\\\', 25), (']', 4), ('^', 322), ('_', 107), ('`', 16), ('a', 726754), ('b', 148176), ('c', 253811), ('d', 319199), ('e', 964237), ('f', 163468), ('g', 191416), ('h', 397259), ('i', 592936), ('j', 23898), ('k', 111404), ('l', 371704), ('m', 225041), ('n', 552588), ('o', 684697), ('p', 184115), ('q', 6356), ('r', 515062), ('s', 585280), ('t', 698276), ('u', 258476), ('v', 81822), ('w', 171901), ('x', 17369), ('y', 209349), ('z', 11610), ('{', 9), ('|', 66), ('}', 12), ('~', 133)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def load_data(data_file):\n",
    "    \"\"\"Load the data into a list of strings\"\"\"\n",
    "    \n",
    "    with open(data_file) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        rows = list(reader)\n",
    "\n",
    "    if data_file == 'train.csv':\n",
    "        sentences, labels = zip(*rows[1:])\n",
    "        sentences = list(sentences)\n",
    "    elif data_file == 'test.csv':\n",
    "        sentences = [row[0] for row in rows[1:]]\n",
    "    else:\n",
    "        print(\"Can only load 'train.csv' or 'test.csv'\")\n",
    "    \n",
    "    # replace non ascii chars to spaces\n",
    "    count = 0\n",
    "    for i, sen in enumerate(sentences):\n",
    "        count = count + sum([0 if ord(i) < 128 else 1 for i in sen])\n",
    "        \n",
    "        # '\\n' indicates the end of the sentence\n",
    "        sentences[i] = ''.join([i if ord(i) < 128 else ' ' for i in sen]) + '\\n'\n",
    "        \n",
    "    print('The total of ', count, 'non-ascii chars are removed \\n')\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def char_to_index(sentence, str_voc):\n",
    "    \"\"\"Convert a string to an array by using the index in the vocabulary\"\"\"\n",
    "    \n",
    "    sen_int = np.array([str_voc.index(c) for c in sentence])\n",
    "    return sen_int\n",
    "\n",
    "def convert_sen_to_data(sentences, str_voc):\n",
    "    \"\"\" Convert a list of strings to a list of numpy arrays\"\"\"\n",
    "    data = [None] * len(sentences)\n",
    "    for i, sen in enumerate(sentences):\n",
    "        data[i] = char_to_index(sen, str_voc)\n",
    "        \n",
    "        # sanity check\n",
    "        #if i < 5:\n",
    "        #    recover = \"\".join([str_voc[k] for k in data[i]])\n",
    "        #    print(recover)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_sentences = load_data('train.csv')\n",
    "\n",
    "# NOTE: you need to use the same vocabulary to handle your test sentences\n",
    "vocabulary = list(set(\"\".join(train_sentences))) \n",
    "vocabulary.sort()\n",
    "str_voc = \"\".join(vocabulary)\n",
    "\n",
    "train_data = convert_sen_to_data(train_sentences, str_voc)\n",
    "\n",
    "\n",
    "num_sen = len(train_data)\n",
    "sen_lengths = [sen.shape[0] for sen in train_data]\n",
    "max_len = max(sen_lengths)\n",
    "min_len = min(sen_lengths)\n",
    "num_chars = sum(sen_lengths)\n",
    "\n",
    "print('Data statistics:')\n",
    "print('Number of sentences: ', num_sen)\n",
    "print('Maximum and minimum sentence lengths:', max_len, min_len)\n",
    "print('Total number of characters:', num_chars)\n",
    "print('Vocabulary size: ', len(vocabulary))\n",
    "\n",
    "uniq, uniq_counts = np.unique(np.concatenate(train_data), return_counts=True)\n",
    "freq = np.zeros_like(uniq_counts)\n",
    "freq[uniq] = uniq_counts\n",
    "\n",
    "print('Chars in vocabulary and their frequencies:')\n",
    "print(list(zip(vocabulary, freq.tolist())))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an RNN and a GRU with tensorflow\n",
    "\n",
    "**Q7 (10 points)** In this problem, you are supposed to train a recurrent neural network to model sentences. Particuarly, your model will receive 10 starting characters and should predict the rest of sentence. The model will be evaluated by per-character cross-entropy loss. You will get \n",
    "* 5 points if your per-character cross-entropy loss is less than 3.13 (the loss by predicting with character frequencies). \n",
    "* 8 points if your per-character cross-entropy loss is less than 2\n",
    "* 10 points if your per-character cross-entropy loss is less than 1.5\n",
    "\n",
    "\\*The performance from a [paper](https://arxiv.org/pdf/1808.04444.pdf) indicates that an LSTM can achieve performance of 1.43 * ln(2) = 0.991. \n",
    "\\*The `zip` program for compressing files roughly can achieve a performances of 3.522 bits per character. It corresponds to a performance of  3.522 * ln(2) = 2.441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 4.0658\n",
      "Epoch 2/4\n",
      "50/50 [==============================] - 3s 53ms/step - loss: 3.6263\n",
      "Epoch 3/4\n",
      "50/50 [==============================] - 3s 52ms/step - loss: 3.5591\n",
      "Epoch 4/4\n",
      "50/50 [==============================] - 3s 52ms/step - loss: 3.5409\n",
      "INFO:tensorflow:Assets written to: rnn_lm.mod/assets\n"
     ]
    }
   ],
   "source": [
    "## Create RNN and train the model\n",
    "## NOTE: you may want to put this part of code in a separate .py file\n",
    "\n",
    "from rnn_lm import masked_lm_loss\n",
    "\n",
    "voc_size = len(str_voc)\n",
    "\n",
    "\n",
    "# You don't have to do padding yourself if your model support varied lengths of sequences. \n",
    "train_mat = tf.keras.preprocessing.sequence.pad_sequences(train_data, maxlen=max_len, \n",
    "                                                         padding='post', truncating='post',\n",
    "                                                         value=-1)\n",
    "# I use a small fraction of data to train the model for a quick demo\n",
    "# You probably want to use all the data\n",
    "train_mat = train_mat[:1600]\n",
    "\n",
    "# prepare the input and the desired output\n",
    "train_x = np.concatenate([- np.ones([train_mat.shape[0], 1]), train_mat[:, :-1]], axis=1)\n",
    "train_y = train_mat\n",
    "\n",
    "\n",
    "# construct the model\n",
    "# Here I include a Lambda layer and an embedding layer for your reference\n",
    "batch_size = 32\n",
    "model_batch = tf.keras.Sequential()\n",
    "model_batch.add(tf.keras.layers.InputLayer(batch_input_shape=(batch_size, 100, 1)))\n",
    "model_batch.add(tf.keras.layers.Lambda(lambda x: tf.squeeze(x + 1, axis=[-1])))\n",
    "model_batch.add(tf.keras.layers.Embedding(input_dim=voc_size + 1, output_dim=10, input_length=max_len))\n",
    "model_batch.add(tf.keras.layers.SimpleRNN(95, activation='tanh', return_sequences=True, stateful=False))\n",
    "\n",
    "# NOTE: the output of the model should be `[batch_size, seq_length, voc_size]`\n",
    "# `seq_length` can be either the original length if you do not pad, or the \n",
    "# length after padding\n",
    "\n",
    "model_batch.compile(optimizer=\"Adam\", loss=masked_lm_loss)\n",
    "model_batch.fit(x=train_x, y=train_y, epochs=4, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# NOTE: The following code converts the trained model to a \"stateful\" one so it can do stepwise \n",
    "# predictions without forgetting previous hidden states. We do this by allocating a\n",
    "# a new model and copying weights from the trained model to this new model. \n",
    "\n",
    "# TODO: you need to do the same thing for your own model. This example only works for this example\n",
    "\n",
    "# NOTE: the batch size needs to be one because your model will be used to generate \n",
    "# a single sentence below. \n",
    "\n",
    "batch_size = 1\n",
    "model = tf.keras.Sequential()\n",
    "# NOTE: You need to use exactly the same way to construct this model as your trained model BUT set \n",
    "# `stateful=True` to EVERY recurrent layer\n",
    "\n",
    "model.add(tf.keras.layers.InputLayer(batch_input_shape=(batch_size, 100, 1)))\n",
    "model.add(tf.keras.layers.Lambda(lambda x: tf.squeeze(x + 1, axis=[-1])))\n",
    "model.add(tf.keras.layers.Embedding(input_dim=voc_size + 1, output_dim=10, input_length=max_len))\n",
    "model.add(tf.keras.layers.SimpleRNN(95, activation='tanh', return_sequences=True, stateful=True))\n",
    "\n",
    "\n",
    "# Then copy weights from the trained model to this new model\n",
    "for il, layer in enumerate(model_batch.layers):\n",
    "    model.layers[il].set_weights(layer.get_weights())\n",
    "\n",
    "\n",
    "model.save('rnn_lm.mod') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total of  1131 non-ascii chars are removed \n",
      "\n",
      "predict and calculate loss:\n",
      "The per-char-loss is about 3.540941\n",
      "WARNING:tensorflow:Model was constructed with shape (1, 100, 1) for input Tensor(\"input_12_1:0\", shape=(1, 100, 1), dtype=float32), but it was called on an input with incompatible shape (1, 1, 1).\n",
      "Difference between the two types of predictions is  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from rnn_lm import masked_lm_loss\n",
    "\n",
    "# load the test data. NOTE: need to use the same vocabulary as the training data\n",
    "sentences = load_data('test.csv')\n",
    "\n",
    "# NOTE: To speed up the testing speed, I randomly select 1000 sentences as the test set. \n",
    "# Let me know if you get a much better performance on the entire test set. \n",
    "np.random.seed(137)\n",
    "selection = np.random.choice(len(sentences), size=1000, replace=False)\n",
    "\n",
    "# prepare the input\n",
    "test_sentences = [sentences[i] for i in selection]\n",
    "test_data = convert_sen_to_data(test_sentences, str_voc)\n",
    "test_mat = tf.keras.preprocessing.sequence.pad_sequences(test_data, maxlen=max_len, \n",
    "                                                         padding='post', truncating='post',\n",
    "                                                         value=-1)\n",
    "\n",
    "test_x = np.concatenate([- np.ones([test_mat.shape[0], 1]), test_mat[:, :-1]], axis=1)\n",
    "\n",
    "# Load your powerful model and compile it with the loss I have defined.\n",
    "# NOTE: compiling your model with my loss should not matter because I only use \n",
    "# your model for prediction. \n",
    "model = tf.keras.models.load_model('rnn_lm.mod', compile=False)\n",
    "model.compile(optimizer=\"adam\", loss=masked_lm_loss)\n",
    "\n",
    "# set batch size to 1\n",
    "batch_size = 1\n",
    "\n",
    "# Evaluate the model on test sentences in batch mode\n",
    "model.reset_states()\n",
    "batch_pred = model.predict(test_x, batch_size=1)\n",
    "losses = masked_lm_loss(test_mat, batch_pred)\n",
    "per_char_loss = np.mean(losses.numpy())\n",
    "\n",
    "# Your points will be decided by the per-char-loss\n",
    "print('predict and calculate loss:')\n",
    "print('The per-char-loss is about %f' % per_char_loss)\n",
    "\n",
    "\n",
    "# make sure that stepwise predictions are the same as batch predictions\n",
    "# test the model on a single sentence\n",
    "\n",
    "test_x_single = test_x[0:1]\n",
    "test_single = test_mat[0:1]\n",
    "\n",
    "# batch prediction\n",
    "model.reset_states()\n",
    "batch_pred = model.predict(test_x_single, batch_size = batch_size)\n",
    "\n",
    "# step-wise prediction\n",
    "model.reset_states()\n",
    "diff = 0\n",
    "for t in range(max_len):\n",
    "        \n",
    "    predict = model.predict(test_x_single[0:1, t:t+1], batch_size=1)\n",
    "       \n",
    "    max_per_entry_diff = np.max(np.abs(predict[0, 0] - batch_pred[0, t]))\n",
    "\n",
    "    if diff < max_per_entry_diff:\n",
    "        diff = max_per_entry_diff\n",
    "\n",
    "# The difference should be zero\n",
    "print('Difference between the two types of predictions is ', diff)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to generate sentences\n",
    "\n",
    "Now we can use the trained model to generate text with a starting string. The naive model just predict frequent characters in the text, so there is no meaningful generation yet. See what you get from your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from \"There \", the generated sentence is:\n",
      "\"There -r,H}n-m,)\"\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, str_voc):\n",
    "    \"\"\" Generate random text from a starting string. The code is modified from this \n",
    "    [example](https://www.tensorflow.org/tutorials/text/text_generation)\"\"\"\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 100 - len(start_string)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = np.array([str_voc.index(s) for s in start_string])\n",
    "    input_eval = np.reshape(input_eval, [1, -1, 1])\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        \n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = predictions[0]\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.reshape([predicted_id], [1, 1, 1])\n",
    "\n",
    "        text_generated.append(str_voc[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "\n",
    "start_string = 'There '\n",
    "gen_sen = generate_text(model, start_string, str_voc)\n",
    "gen_sen = gen_sen.split('\\n')[0]\n",
    "\n",
    "print('Starting from \"' + start_string + '\", the generated sentence is:')\n",
    "print('\"' + gen_sen + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
