{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network for Modeling Sentences\n",
    "\n",
    "In this task, we will use RNNs to model sentences. The task is to predict the next character in a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total of  4328 non-ascii chars are removed \n",
      "\n",
      "Data statistics:\n",
      "Number of sentences:  160000\n",
      "Maximum and minimum sentence lengths: 100 32\n",
      "Total number of characters: 10954565\n",
      "Vocabulary size:  95\n",
      "Chars in vocabulary and their frequencies:\n",
      "[('l', 371704), ('-', 20064), ('D', 6787), ('q', 6356), ('#', 496), ('F', 3232), ('!', 12100), ('y', 209349), ('7', 2496), ('_', 107), ('T', 15062), (':', 22223), ('m', 225041), ('H', 11482), ('Y', 2381), ('>', 9), (',', 33680), ('V', 720), ('$', 1212), ('W', 37161), (';', 607), ('&', 1366), ('S', 7281), ('B', 4063), ('.', 108694), ('k', 111404), ('+', 123), ('b', 148176), ('<', 12), ('n', 552588), ('3', 3517), ('P', 3722), ('=', 103), ('p', 184115), ('N', 3017), ('a', 726754), ('c', 253811), ('?', 48816), ('R', 2942), ('t', 698276), ('[', 1), ('h', 397259), (' ', 1762678), ('o', 684697), ('f', 163468), ('^', 322), ('{', 9), ('0', 11139), ('@', 34), ('~', 133), (\"'\", 88729), ('4', 2882), ('Q', 1036), ('j', 23898), ('|', 66), ('g', 191416), (')', 8890), ('w', 171901), ('5', 4272), ('O', 2211), ('*', 4310), ('A', 8259), ('6', 2673), ('X', 17), ('9', 2801), ('v', 81822), ('I', 15839), ('/', 1586), ('C', 5317), ('%', 450), ('G', 2668), ('r', 515062), (']', 4), ('e', 964237), ('E', 2239), ('\\n', 160000), ('K', 2315), ('x', 17369), ('M', 7724), ('U', 1014), ('8', 2071), ('(', 8734), ('J', 2999), ('L', 2612), ('i', 592936), ('z', 11610), ('d', 319199), ('`', 16), ('1', 10960), ('\\\\', 25), ('Z', 149), ('u', 258476), ('s', 585280), ('2', 7690), ('}', 12)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def load_data(data_file):\n",
    "    \"\"\"Load the data into a list of strings\"\"\"\n",
    "    \n",
    "    with open(data_file) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        rows = list(reader)\n",
    "\n",
    "    if data_file == 'train.csv':\n",
    "        sentences, labels = zip(*rows[1:])\n",
    "        sentences = list(sentences)\n",
    "    elif data_file == 'test.csv':\n",
    "        sentences = [row[0] for row in rows[1:]]\n",
    "    else:\n",
    "        print(\"Can only load 'train.csv' or 'test.csv'\")\n",
    "    \n",
    "    # replace non ascii chars to spaces\n",
    "    count = 0\n",
    "    for i, sen in enumerate(sentences):\n",
    "        count = count + sum([0 if ord(i) < 128 else 1 for i in sen])\n",
    "        \n",
    "        # '\\n' indicates the end of the sentence\n",
    "        sentences[i] = ''.join([i if ord(i) < 128 else ' ' for i in sen]) + '\\n'\n",
    "        \n",
    "    print('The total of ', count, 'non-ascii chars are removed \\n')\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def char_to_index(sentence, str_voc):\n",
    "    \"\"\"Convert a string to an array by using the index in the vocabulary\"\"\"\n",
    "    \n",
    "    sen_int = np.array([str_voc.index(c) for c in sentence])\n",
    "    return sen_int\n",
    "\n",
    "def convert_sen_to_data(sentences, str_voc):\n",
    "    \"\"\" Convert a list of strings to a list of numpy arrays\"\"\"\n",
    "    data = [None] * len(sentences)\n",
    "    for i, sen in enumerate(sentences):\n",
    "        data[i] = char_to_index(sen, str_voc)\n",
    "        \n",
    "        # sanity check\n",
    "        #if i < 5:\n",
    "        #    recover = \"\".join([str_voc[k] for k in data[i]])\n",
    "        #    print(recover)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_sentences = load_data('train.csv')\n",
    "\n",
    "# NOTE: you need to use the same vocabulary to handle your test sentences\n",
    "vocabulary = list(set(\"\".join(train_sentences))) \n",
    "str_voc = \"\".join(vocabulary)\n",
    "\n",
    "train_data = convert_sen_to_data(train_sentences, str_voc)\n",
    "\n",
    "\n",
    "num_sen = len(train_data)\n",
    "sen_lengths = [sen.shape[0] for sen in train_data]\n",
    "max_len = max(sen_lengths)\n",
    "min_len = min(sen_lengths)\n",
    "num_chars = sum(sen_lengths)\n",
    "\n",
    "print('Data statistics:')\n",
    "print('Number of sentences: ', num_sen)\n",
    "print('Maximum and minimum sentence lengths:', max_len, min_len)\n",
    "print('Total number of characters:', num_chars)\n",
    "print('Vocabulary size: ', len(vocabulary))\n",
    "\n",
    "uniq, uniq_counts = np.unique(np.concatenate(train_data), return_counts=True)\n",
    "freq = np.zeros_like(uniq_counts)\n",
    "freq[uniq] = uniq_counts\n",
    "\n",
    "print('Chars in vocabulary and their frequencies:')\n",
    "print(list(zip(vocabulary, freq.tolist())))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an RNN and a GRU with tensorflow\n",
    "\n",
    "**Q7 (10 points)** In this problem, you are supposed to train a recurrent neural network to model sentences. Particuarly, your model will receive 10 starting characters and should predict the rest of sentence. The model will be evaluated by per-character cross-entropy loss. You will get \n",
    "* 5 points if your per-character cross-entropy loss is less than 3.13 (the loss by predicting with character frequencies). \n",
    "* 8 points if your per-character cross-entropy loss is less than 2\n",
    "* 10 points if your per-character cross-entropy loss is less than 1.5\n",
    "\n",
    "\\*The performance from a [paper](https://arxiv.org/pdf/1808.04444.pdf) indicates that an LSTM can achieve performance of 1.43 * ln(2) = 0.991. \n",
    "\\*The `zip` program for compressing files roughly can achieve a performances of 3.522 bits per character. It corresponds to a performance of  3.522 * ln(2) = 2.441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rnn_lm.mod/assets\n"
     ]
    }
   ],
   "source": [
    "## Create RNN and train the model\n",
    "## NOTE: you may want to put this part of code in a separate .py file\n",
    "\n",
    "class StubLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"A stub model, which just returns zero. \"\"\"\n",
    "  def __init__(self, max_len, voc_size, freq):\n",
    "    super(StubLayer, self).__init__()\n",
    "    self.max_len = max_len\n",
    "    self.voc_size = voc_size\n",
    "    self.log_freq = np.log(freq.astype(np.float32))\n",
    "    \n",
    "    self.dense1 = tf.keras.layers.Dense(self.voc_size)\n",
    "\n",
    "  def call(self, inputs):  \n",
    "    \"\"\"Predict logits for the next character \n",
    "    args:\n",
    "        inputs: a integer tensor with shape (1,) indicating the previous character.\n",
    "                Also include hidden states specified by the model\n",
    "        \n",
    "    returns:\n",
    "        outputs: a tensor with shape (voc_size, )\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    naive_pred = tf.constant(self.log_freq)\n",
    "    \n",
    "    outputs = self.dense1(inputs) * 0 + naive_pred\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "\n",
    "class NaiveCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, freq, units=freq.shape[0]):\n",
    "        super(NaiveCell, self).__init__()\n",
    "        \n",
    "        self.log_freq = np.log(freq.astype(np.float32))\n",
    "    \n",
    "        self.state_size = units \n",
    "        self.output_size = units\n",
    "        \n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        output = prev_output * 0 + self.log_freq\n",
    "        return output, [output]\n",
    "\n",
    "cell = NaiveCell(freq=freq)\n",
    "\n",
    "naive_rnn = tf.keras.layers.RNN(cell)\n",
    "\n",
    "\n",
    "# define a stub model and save it\n",
    "# NOTE: in prediction, the input to the model is a matrix of shape [batch_size, max_len+1]. \n",
    "# The first column of the matrix is always a space `str_voc.index(' ')`. \n",
    "# The output of the model should has shape [batch_size, max_len, voc_size]\n",
    "\n",
    "#model = tf.keras.Sequential()\n",
    "#model.add(tf.keras.layers.InputLayer(input_shape=(None, 1,)))\n",
    "#model.add(StubLayer(max_len=max_len, voc_size=len(str_voc), freq=freq))\n",
    "#model.add(naive_rnn)\n",
    "#model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "naive_rnn = tf.keras.layers.SimpleRNN(freq.shape[0], activation='linear',\n",
    "                                 kernel_initializer=tf.keras.initializers.Zeros(),\n",
    "                                 recurrent_initializer=tf.keras.initializers.Zeros(), \n",
    "                                 bias_initializer=tf.keras.initializers.Constant(np.log(freq)))\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(None, 1,)))\n",
    "model.add(naive_rnn)\n",
    "model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "model.save('rnn_lm.mod') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total of  1131 non-ascii chars are removed \n",
      "\n",
      "Number of test instances: 40000\n",
      "Evaluating the model ...\n",
      "The total number of chars in the test set is  2739550\n",
      "The per-char-loss is 3.130\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the test data. NOTE: need to use the same vocabulary as the training data\n",
    "test_sentences = load_data('test.csv')\n",
    "test_data = convert_sen_to_data(test_sentences, str_voc)\n",
    "\n",
    "\n",
    "print('Number of test instances:', len(test_data))\n",
    "\n",
    "# TODO: replace this stub model with your powerful model\n",
    "model = tf.keras.models.load_model('rnn_lm.mod')\n",
    "\n",
    "test_mat = tf.keras.preprocessing.sequence.pad_sequences(test_data, maxlen=max_len, \n",
    "                                                         padding='post', truncating='post',\n",
    "                                                         value=-1)\n",
    "padding_flag = (test_mat < 0).astype(np.int32)\n",
    "\n",
    "print('Evaluating the model ...')\n",
    "\n",
    "loss_sum = 0\n",
    "\n",
    "model.reset_states()\n",
    "for t in range(max_len):\n",
    "\n",
    "    # the input is -1 at beginning and then the character in the previous step afterwards\n",
    "    if t == 0:\n",
    "        char_input = - np.ones([test_mat.shape[0], 1, 1])\n",
    "    else:\n",
    "        char_input = np.reshape(test_mat[:, t - 1], [test_mat.shape[0], 1, 1])\n",
    "        \n",
    "    # predict is a matrix with shape [test_set_size, voc_size]\n",
    "    predicts = model(char_input)\n",
    "    \n",
    "    # groundtruth is the character at the current step\n",
    "    cur_chars = test_mat[:, t]\n",
    "    cur_chars[cur_chars < 0] = 0 # change -1 to 0 since -1 is illegal for one-hot encoding\n",
    "    labels = tf.one_hot(cur_chars, depth=len(str_voc))\n",
    "    \n",
    "    \n",
    "    step_loss = tf.nn.softmax_cross_entropy_with_logits(labels, predicts)\n",
    "    step_loss = step_loss * (1 - padding_flag[:, t]) # only count loss that are not padding characters\n",
    "    \n",
    "    loss_sum = loss_sum + np.sum(step_loss)\n",
    "\n",
    "char_count = np.sum(1 - padding_flag)\n",
    "per_char_loss = loss_sum / char_count\n",
    "\n",
    "print('The total number of chars in the test set is ', char_count)\n",
    "\n",
    "print('The per-char-loss is %.3f' % per_char_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to generate sentences\n",
    "\n",
    "Now we can use the trained model to generate text with a starting string. The naive model just predict frequent characters in the text, so there is no meaningful generation yet. See what you get from your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from \"I have\", the generated sentence is:\n",
      "\"I have'w pta'evaoteedetyt eraolc ' oh etusorilnzduafhn.sugy rcsw lnsesismo  wgq th utrtowp :ad actth\"\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, str_voc):\n",
    "    \"\"\" Generate random text from a starting string. The code is modified from this \n",
    "    [example](https://www.tensorflow.org/tutorials/text/text_generation)\"\"\"\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 100 - len(start_string)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = np.array([str_voc.index(s) for s in start_string])\n",
    "    input_eval = np.reshape(input_eval, [1, -1, 1])\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        \n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = predictions[0]\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions[None, :], num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.reshape([predicted_id], [1, 1, 1])\n",
    "\n",
    "        text_generated.append(str_voc[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "\n",
    "start_string = 'I have'\n",
    "gen_sen = generate_text(model, start_string, str_voc)\n",
    "gen_sen = gen_sen.split('\\n')[0]\n",
    "\n",
    "print('Starting from \"' + start_string + '\", the generated sentence is:')\n",
    "print('\"' + gen_sen + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
